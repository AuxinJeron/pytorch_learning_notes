{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Materials**\n",
    "- https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following sample deep learning model.\n",
    "\n",
    "\n",
    "The input can be represented by a matrix of size $(n, Di)$, where $n$ is the sample size, $Di$ is the number of input features.   \n",
    "\n",
    "\n",
    "$$\\newcommand{\\matrixX}{\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1 & \\cdots &\n",
    "x^{(1)}_{Di} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x^{(n)}_1 & \\cdots &\n",
    "x^{(n)}_{Di}\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\mathbf{X}\n",
    "=\n",
    "\\matrixX\n",
    "$$ \n",
    "\n",
    "The predicated output can be represented by a matrix of size $(n, Do)$, where $Do$ is the number of output features.\n",
    "\n",
    "$$\\newcommand{\\matrixY}{\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}_1 & \\cdots &\n",
    "y^{(1)}_{Do} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "y^{(n)}_1 & \\cdots &\n",
    "y^{(n)}_{Do}\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\mathbf{Y}\n",
    "=\n",
    "\\matrixY\n",
    "$$\n",
    "\n",
    "The actual output is represented as following matrix\n",
    "\n",
    "$$\\newcommand{\\matrixActualY}{\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}^{(1)}_1 & \\cdots &\n",
    "\\hat{y}^{(1)}_{Do} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\hat{y}^{(n)}_1 & \\cdots &\n",
    "\\hat{y}^{(n)}_{Do}\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\mathbf{\\hat{Y}}\n",
    "=\n",
    "\\matrixActualY\n",
    "$$\n",
    "\n",
    "Assume there is only one hidde layer with $H$ neurons. So the intermediate values can be represented by a matrix of size $(n, H)$.\n",
    "\n",
    "$$\\newcommand{\\matrixZ}{\n",
    "\\begin{bmatrix}\n",
    "z^{(1)}_1 & \\cdots &\n",
    "z^{(1)}_{H} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "z^{(n)}_1 & \\cdots &\n",
    "z^{(n)}_{H}\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\mathbf{Z}\n",
    "=\n",
    "\\matrixZ\n",
    "$$\n",
    "\n",
    "The activation results of this hidden layer can be represented by a matrix of size $(n, H)$ too.\n",
    "\n",
    "$$\\newcommand{\\matrixR}{\n",
    "\\begin{bmatrix}\n",
    "r^{(1)}_1 & \\cdots &\n",
    "r^{(1)}_{H} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "r^{(n)}_1 & \\cdots &\n",
    "r^{(n)}_{H}\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\mathbf{R}\n",
    "=\n",
    "\\matrixR\n",
    "$$\n",
    "\n",
    "So the weights between input and hidden layer is\n",
    "\n",
    "$$\\newcommand{\\matrixWone}{\n",
    "\\begin{bmatrix}\n",
    "w^{(1)}_{1, 1} & \\cdots &\n",
    "w^{(1)}_{1, H} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "w^{(1)}_{Di, 1} & \\cdots &\n",
    "w^{(1)}_{Di, H}\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\mathbf{W1}\n",
    "=\n",
    "\\matrixWone\n",
    "$$\n",
    "\n",
    "The weights between hidden layer and output is \n",
    "\n",
    "$$\\newcommand{\\matrixWtwo}{\n",
    "\\begin{bmatrix}\n",
    "w^{(2)}_{1, 1} & \\cdots &\n",
    "w^{(2)}_{1, Do} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "w^{(2)}_{H, 1} & \\cdots &\n",
    "w^{(2)}_{H, Do}\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\mathbf{W2}\n",
    "=\n",
    "\\matrixWtwo\n",
    "$$\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Z} &= \\mathbf{X} \\cdot \\mathbf{W1} = \\matrixX \\cdot \\matrixWone \\\\\n",
    "\\\\\n",
    "\\mathbf{R} &= relu(\\mathbf{Z})\n",
    "\\\\\n",
    "\\mathbf{Y} &= \\mathbf{R} \\cdot \\mathbf{W2} = \\matrixR \\cdot \\matrixWtwo \\\\\n",
    "\\\\\n",
    "\\mathbf{C} &= \\sum_{u=1}^{n}\\sum_{i=1}^{Do}(\\hat{y}^{(u)}_i - y^{(u)}_i)^2 = sum(square(\\mathbf{\\hat{Y}} - \\mathbf{Y}))\n",
    "\\end{align*}\n",
    "\n",
    "### Backpropagation pass\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Y}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "2 \\cdot (\\hat{y}^{(1)}_1 - y^{(1)}_1) & \\cdots &\n",
    "2 \\cdot (\\hat{y}^{(1)}_{Do} - y^{(1)}_{Do}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "2 \\cdot (\\hat{y}^{(n)}_1 - y^{(n)}_1) & \\cdots &\n",
    "2 \\cdot (\\hat{y}^{(n)}_{Do} - y^{(n)}_{Do}) \n",
    "\\end{bmatrix} \n",
    "= 2 \\cdot (\\matrixActualY - \\matrixY) = 2 \\cdot (\\mathbf{\\hat{Y}} - \\mathbf{Y}) \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{\\partial \\mathbf{C^{(i)}}}{\\partial \\mathbf{W2}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial w^{(2)}_{1, 1}} & \\cdots &\n",
    "\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial w^{(2)}_{1, Do}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial w^{(2)}_{H, 1}} & \\cdots &\n",
    "\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial w^{(2)}_{H, Do}} \n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum_{j=1}^{Do}{\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial y^{(i)}_{j}} \\cdot \\frac{\\partial y^{(i)}_{j}}{\\partial w^{(2)}_{1, 1}}} & \\cdots &\n",
    "\\sum_{j=1}^{Do}{\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial y^{(i)}_{j}} \\cdot \\frac{\\partial y^{(i)}_{j}}{\\partial w^{(2)}_{1, Do}}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_{j=1}^{Do}{\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial y^{(i)}_{j}} \\cdot \\frac{\\partial y^{(i)}_{j}}{\\partial w^{(2)}_{H, 1}}} & \\cdots \n",
    "&\n",
    "\\sum_{j=1}^{Do}{\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial y^{(i)}_{j}} \\cdot \\frac{\\partial y^{(i)}_{j}}{\\partial w^{(2)}_{H, Do}}} \n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial y^{(i)}_1} \\cdot \\frac{\\partial y^{(i)}_{1}}{\\partial w^{(2)}_{1, 1}} & \\cdots &\n",
    "\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial y^{(i)}_{Do}} \\cdot \\frac{\\partial y^{(i)}_{Do}}{\\partial w^{(2)}_{1, Do}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial y^{(i)}_1} \\cdot \\frac{\\partial y^{(i)}_1}{\\partial w^{(2)}_{H, 1}} & \\cdots &\n",
    "\\frac{\\partial \\mathbf{C}^{(i)}}{\\partial y^{(i)}_{Do}} \\cdot \\frac{\\partial y^{(i)}_{Do}}{\\partial w^{(2)}_{H, Do}} \n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\mathbf{Z}^{(1)T} \\cdot \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Y}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{W2}}\n",
    "&=\n",
    "\\mathbf{Z}^{T} \\cdot \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{Y}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Proof Relatefd\n",
    "- Multi variable chain rule\n",
    "    - https://www.youtube.com/watch?v=NO3AqAaAE6o\n",
    "- Total differential\n",
    "- Jacobian, chain rule\n",
    "    - https://suzyahyah.github.io/calculus/machine%20learning/2018/04/04/Jacobian-and-Backpropagation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 572.4231396454019\n",
      "199 4.538714631495535\n",
      "299 0.05687570705846841\n",
      "399 0.0008129116907557357\n",
      "499 1.2465652749864821e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randonly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicated Y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # Backprop to compuate gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # TODO: How does the following equation come? \n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 352.47174072265625\n",
      "199 0.7235959768295288\n",
      "299 0.0031361570581793785\n",
      "399 0.00010471556015545502\n",
      "499 2.3665821572649293e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N is batch size; D_in is input dimension\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicated y\n",
    "    h = x.mm(w1) # .mm is matrix multiplication\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # Compute and print loss \n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 264.5614013671875\n",
      "199 0.40666186809539795\n",
      "299 0.0011039602104574442\n",
      "399 5.3668794862460345e-05\n",
      "499 1.6031990526244044e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# N is batch size; D_in is input dimension\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 245.68959045410156\n",
      "199 1.0950459241867065\n",
      "299 0.01042528823018074\n",
      "399 0.0002860032254830003\n",
      "499 4.321546293795109e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "    \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our function, we use Function.apply method. We alias this as `relu`\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    # Forward pass: compute predicated y using operation; we compute\n",
    "    # ReLU using our custom autograd operation\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.7842166423797607\n",
      "199 0.0480002835392952\n",
      "299 0.00136263866443187\n",
      "399 4.541453017736785e-05\n",
      "499 1.6755162732806639e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "# Mean Squared Error\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # Zero the gradients before running the backward pass\n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 47.444175720214844\n",
      "199 0.6619318127632141\n",
      "299 0.0026850791182368994\n",
      "399 1.0386150279373396e-05\n",
      "499 2.7777673494711053e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "# Mean Squared Error\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom nn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 45.171382904052734\n",
      "199 0.4758076071739197\n",
      "299 0.002161941723898053\n",
      "399 2.9479062959580915e-06\n",
      "499 7.16484871432499e-10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "\n",
    "model = TwoLayerNet(D_in,  H, D_out)\n",
    "\n",
    "# Mean Squared Error\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Flow + Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
